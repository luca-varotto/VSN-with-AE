# -*- coding: utf-8 -*-
"""VSN_triggering_MC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17tsWwIxtb6mGGKmcD197sbdOwyVUEq-6
"""

from google.colab import drive
ROOT = "/content/drive/"
drive.mount(ROOT)

import numpy as np
from os.path import join
from importlib.machinery import SourceFileLoader
from sklearn.utils.testing import ignore_warnings
from sklearn.exceptions import ConvergenceWarning
from warnings import simplefilter
simplefilter("ignore", category=ConvergenceWarning)
from tqdm import tqdm
from datetime import datetime
import matplotlib.pyplot as plt
from statsmodels.distributions.empirical_distribution import ECDF

PROJ = "My Drive/Colab Notebooks/P13-VSN with AE/"
PROJECT_PATH = join(ROOT, PROJ)
clusterizer = SourceFileLoader('somelib', join(PROJECT_PATH, 'clusterizer.py')).load_module()
DEC = SourceFileLoader('somelib', join(PROJECT_PATH, 'DEC.py')).load_module()

# SETUP PARAMETERS

# define the number of events and their a-priori probability
M = 3   
events = range(M)
p_events = (1/M)*np.ones(M) # suppose uniform distribution over the events

# define the number of nodes
N = 30

# maximum number of events
M_max = 10

# to test the curse of dimensionality and data overfitting, a MC experiment is performed
# Define the number of tests over the MC experiment 
num_MCtests = 20
# some storage for GMM
M_gmm = np.zeros(M_max, dtype=int)
M_eff_gmm = np.zeros(M_max, dtype=int)
acc_gmm_MC = []
weights_gmm = np.zeros((M_max,num_MCtests))
weights_eff_gmm = np.zeros((M_max,num_MCtests))
rec_err_gmm = []
# some storage for V-GMM
M_vgmm = np.zeros(M_max, dtype=int)
M_eff_vgmm = np.zeros(M_max, dtype=int)
acc_vgmm_MC = []
weights_vgmm = np.zeros((M_max,num_MCtests))
weights_eff_vgmm = np.zeros((M_max,num_MCtests))
rec_err_vgmm = []
# some storage for GMM+AE
M_gmm_DEC = np.zeros(M_max, dtype=int)
M_eff_gmm_DEC = np.zeros(M_max, dtype=int)
acc_gmm_DEC_MC = []
weights_gmm_DEC = np.zeros((M_max,num_MCtests))
weights_eff_gmm_DEC = np.zeros((M_max,num_MCtests))
rec_err_gmm_DEC = []
# some storage for V-GMM+AE
M_vgmm_DEC = np.zeros(M_max, dtype=int)
M_eff_vgmm_DEC = np.zeros(M_max, dtype=int)
acc_vgmm_DEC_MC = []
weights_vgmm_DEC = np.zeros((M_max,num_MCtests))
weights_eff_vgmm_DEC = np.zeros((M_max,num_MCtests))
rec_err_vgmm_DEC = []

for test_num in range(num_MCtests):

  print("MC simulation %d / %d " %(test_num,num_MCtests-1) )

  # define the triggering model
  scale_separator = 1
  tau = scale_separator*np.random.randint(0,2,size=(M,N)) # m-th row has the nodes triggerd by the m-th event
  while np.linalg.matrix_rank(tau) < M: # suppose M < N
    tau = scale_separator*np.random.randint(0,2,size=(M,N))
  # explore underlying triggering structure
  # print("\ntau = \n", tau)

  # GENERATE MEASUREMENTS (DATASET)

  # define the success probability for each node (success = correct measurement)
  p_succ = 0.8

  # number of measurements
  n = 10000
  h = np.empty((n,N)) # i-th row is the i-th measurement (N-dim binary vector)
  labels = [] # collect the groundtruth labels (i.e., triggering events)
  confidence_range = [0.7,1.0]
  for i in tqdm(range(n)): 
    # randomly choose one of the M events acting at the i-th measurement
    event = np.random.choice(events, 1, p=p_events)
    labels.append(event)
    v_i = np.zeros(N) # noise
    for node in range(N):
      if not np.random.binomial(1,p_succ): # success is a Bernoulli process of success probability p_succ
        v_i[node] = scale_separator if np.random.binomial(1,0.5) else -scale_separator
        # v_i[node] *= np.random.uniform(confidence_error_range[0],confidence_error_range[1]) # smooth the error by the detection confidence
    h[i,:] = np.multiply(np.minimum(np.maximum( tau[event,:]+ v_i,0),scale_separator),np.random.uniform(confidence_range[0],confidence_range[1],N))
  # h = np.minimum(h,scale_separator)
  # explore measurements
  # print("h= \n", h)
  # fig = plt.figure(figsize=(10,10))
  # ax = fig.add_subplot(111, projection='3d')
  # plt.scatter(h[:, 0], h[:, 1], s=50)

  # CLUSTERING OF MEASUREMENTS
  M_range = range(1,M_max+1)
  # GMM 
  gmm = clusterizer.GMM()  
  gmm = clusterizer.GMM(gmm.GMM_generation(h, M_range,type='standard', want_to_plot= False)) # fit on data
  labels_gmm = gmm.prediction(h) # prediction
  cm_gmm,acc_gmm,ind_gmm = gmm.confusion_matrix(labels, labels_gmm, False) # confusion matrix
  # reordering accordin to the confusion martix
  gmm.gmm_model.weights_ = gmm.gmm_model.weights_[ind_gmm[:,1]]
  gmm.gmm_model.means_ = gmm.gmm_model.means_[ind_gmm[:,1]]
  gmm.gmm_model.covariances_ = gmm.gmm_model.covariances_[ind_gmm[:,1]]
  # store results
  acc_gmm_MC.append(acc_gmm)
  M_gmm[np.array(labels_gmm).max()] += 1
  weights_gmm[:len(gmm.gmm_model.weights_),test_num] = gmm.gmm_model.weights_
  # print("GMM acc: %5.3f" %(acc_gmm))

  # V-GMM 
  vgmm = clusterizer.GMM()  
  vgmm = clusterizer.GMM(vgmm.GMM_generation(h, M_range,type='bayesian', weight_concentration_prior_type='dirichlet_process', weight_concentration_prior=1.0E-7, want_to_plot= False)) # fit on data
  labels_vgmm = vgmm.prediction(h) # prediction
  cm_vgmm,acc_vgmm,ind_vgmm = vgmm.confusion_matrix(labels, labels_vgmm, False) # confusion matrix
  # reordering accordin to the confusion martix
  vgmm.gmm_model.weights_ = vgmm.gmm_model.weights_[ind_vgmm[:,1]]
  vgmm.gmm_model.means_ = vgmm.gmm_model.means_[ind_vgmm[:,1]]
  vgmm.gmm_model.covariances_ = vgmm.gmm_model.covariances_[ind_vgmm[:,1]]
  # store results
  acc_vgmm_MC.append(acc_vgmm)
  M_vgmm[np.array(labels_vgmm).max()] += 1
  weights_vgmm[:len(vgmm.gmm_model.weights_),test_num] = vgmm.gmm_model.weights_
  # print("V-GMM acc: %5.3f" %(acc_vgmm))

  # Kmeans
  # K_range = M_range
  # kmeans = clusterizer.kMeans()  
  # models = kmeans.kmeans_generation(h,K_range, True) # fit models on data
  # best_model_idx = M#-1
  # while best_model_idx not in K_range:
  #   best_model_idx = int(input("Number of clusters? "))
  # kmeans = clusterizer.kMeans(models[best_model_idx-1]) # select best model
  # labels_kmeans = kmeans.prediction(h) # prediction
  # cm_kmeans, acc_kmeans, ind_kmeans = kmeans.confusion_matrix(labels, labels_kmeans, True) # confusion matrix
  # print("KMeans acc: %5.3f" %(acc_kmeans))

  # AUTOENCODER

  # load existing model
  loaded_model = None
  # model_name =""#'./models/AE05042020-083811.h5'
  # try:
  #   loaded_model = keras.models.load_model(model_name) # load pre-traiend model
  #   loaded_scaler =   # load scaler
  # except:
  #   loaded_model = None

  # define the encoder structure (decoder is symmetric)
  structure_array = [int(3*N/4),int(N/2),int(N/4),2]
  # create the autoencoder model (autoencoder, encoder, decoder)
  ae = DEC.AE(N,structure_array=structure_array,loaded_model=loaded_model) 
  if loaded_model is None: # if no model is loaded, we need to train a new one
    X_train, X_test, y_train, y_test, scaler = ae.data_preproc(h,labels) # train/test splittint
    print("X_train shape: {} \nX_test shape: {} \ny_train shape: {} \ny_test shape: {} \n".format(\
          np.shape(X_train),np.shape(X_test),np.shape(y_train),np.shape(y_test)))
    epochs = 15
    batch_size = 30
    history = ae.AEtrain(X_train, X_test,epochs, batch_size) # train 
    ae.plot_history(history) # plot metrics
    # save model 
    now = datetime.now()
    date_time = now.strftime("%m%d%Y-%H%M%S")
    model_name = './models/' +  'AE' + date_time  +'.h5'
    # ae.AEsave(model_name) # save
    # ... save scaler
  else:
    X_test = scaler.transform(h)
    y_test = labels
  # test the model
  encoded, decoded,reconstructed_data = ae.AEpredict(X_test) 

  # CLUSTERING ON EMBEDDED FEATURES
  # GMM 
  gmm_DEC = clusterizer.GMM()  
  gmm_DEC = clusterizer.GMM(gmm_DEC.GMM_generation(encoded, M_range,type='standard', want_to_plot= False)) # fit on data
  labels_gmm_DEC = gmm_DEC.prediction(encoded, want_to_plot=False) # prediction
  cm_gmm_DEC,acc_gmm_DEC,ind_gmm_DEC = gmm_DEC.confusion_matrix(y_test, labels_gmm_DEC, False) # confusion matrix
  # reordering accordin to the confusion martix
  gmm_DEC.gmm_model.weights_ = gmm_DEC.gmm_model.weights_[ind_gmm_DEC[:,1]]
  gmm_DEC.gmm_model.means_ = gmm_DEC.gmm_model.means_[ind_gmm_DEC[:,1]]
  gmm_DEC.gmm_model.covariances_ = gmm_DEC.gmm_model.covariances_[ind_gmm_DEC[:,1]]
  # store results
  acc_gmm_DEC_MC.append(acc_gmm_DEC)
  M_gmm_DEC[np.array(labels_gmm_DEC).max()] += 1
  weights_gmm_DEC[:len(gmm_DEC.gmm_model.weights_),test_num] = gmm_DEC.gmm_model.weights_
  # print("GMM acc: %5.3f" %(acc_gmm_DEC))

  # V-GMM 
  vgmm_DEC = clusterizer.GMM()  
  vgmm_DEC = clusterizer.GMM(vgmm_DEC.GMM_generation(encoded, M_range,type='bayesian', weight_concentration_prior_type='dirichlet_process', weight_concentration_prior=1.0E-7, want_to_plot= True)) # fit on data
  labels_vgmm_DEC = vgmm_DEC.prediction(encoded,want_to_plot=False) # prediction
  # reordering accordin to the confusion martix
  cm_vgmm_DEC,acc_vgmm_DEC,ind_vgmm_DEC = vgmm_DEC.confusion_matrix(y_test, labels_vgmm_DEC, False) # confusion matrix
  vgmm_DEC.gmm_model.weights_ = vgmm_DEC.gmm_model.weights_[ind_vgmm_DEC[:,1]]
  vgmm_DEC.gmm_model.means_ = vgmm_DEC.gmm_model.means_[ind_vgmm_DEC[:,1]]
  vgmm_DEC.gmm_model.covariances_ = vgmm_DEC.gmm_model.covariances_[ind_vgmm_DEC[:,1]]
  # store results 
  acc_vgmm_DEC_MC.append(acc_vgmm_DEC)
  M_vgmm_DEC[np.array(labels_vgmm_DEC).max()] += 1
  weights_vgmm_DEC[:len(vgmm_DEC.gmm_model.weights_),test_num] = vgmm_DEC.gmm_model.weights_
  # print("V-GMM acc: %5.3f" %(acc_vgmm_DEC))

  # # Kmeans
  # K_range = M_range
  # kmeans_DEC = clusterizer.kMeans()  
  # models = kmeans_DEC.kmeans_generation(encoded,K_range, True) # fit models on data
  # best_model_idx = M#-1
  # while best_model_idx not in K_range:
  #   best_model_idx = int(input("Number of clusters? "))
  # kmeans_DEC = clusterizer.kMeans(models[best_model_idx-1]) # select best model
  # labels_kmeans_DEC = kmeans_DEC.prediction(encoded,want_to_plot=True) # prediction
  # cm_kmeans_DEC, acc_kmeans_DEC, ind_kmeans_DEC = kmeans_DEC.confusion_matrix(y_test, labels_kmeans_DEC, True) # confusion matrix
  # print("KMeans-DEC acc: %5.3f" %(acc_kmeans_DEC))

  # HYPERCUBE REPROJECTION 
  # map each cluster centroid to the closest hypercube vertex and discover how many EFFECTIVE clusters are
  # detected and how many of these coincide with the TRUE ones (reconstruction accuracy)

  # GMM
  mu_gmm_projected = np.round(gmm.gmm_model.means_).astype(int)
  gmm2tau = np.zeros(M,dtype=int) # how many clusters are projected onto the true vertices
  gmm_err = 0 # how many new (fake) clusters are created
  gmm_tau = {} # dictionary of reconstructed hypercube vertices (keys) and corresponding weights (values)
  for k in range(len(mu_gmm_projected)):
    mu = mu_gmm_projected[k]
    wrong_reconstruction = True
    for m in range(M):
      if np.count_nonzero(tau[m,:] != mu) == 0:
        gmm2tau[m] += 1
        if str(tau[m,:]) not in gmm_tau.keys():
          gmm_tau[str(tau[m,:])] = gmm.gmm_model.weights_[k]
        else:
          gmm_tau[str(tau[m,:])] += gmm.gmm_model.weights_[k]
        wrong_reconstruction = False
    if wrong_reconstruction:
      gmm_err += gmm.gmm_model.weights_[k]
      if str(mu) not in gmm_tau.keys():
        gmm_tau[str(mu)] = gmm.gmm_model.weights_[k]
      else:
        gmm_tau[str(mu)] += gmm.gmm_model.weights_[k]
  rec_err_gmm.append(gmm_err)
  M_eff_gmm[len(list(gmm_tau.values()))-1] += 1
  weights_eff_gmm[:len(list(gmm_tau.values())),test_num] = list(gmm_tau.values())

  # V-GMM
  mu_vgmm_projected = np.round(vgmm.gmm_model.means_).astype(int)
  vgmm2tau = np.zeros(M,dtype=int) # how many clusters are projected onto the true vertices
  vgmm_err = 0 # how many new (fake) clusters are created
  vgmm_tau = {} # dictionary of reconstructed hypercube vertices (keys) and corresponding weights (values)
  for k in range(len(mu_vgmm_projected)):
    mu = mu_vgmm_projected[k]
    wrong_reconstruction = 1
    for m in range(M):
      if np.count_nonzero(tau[m,:] != mu) == 0:
        vgmm2tau[m] += 1
        if str(tau[m,:]) not in vgmm_tau.keys():
          vgmm_tau[str(tau[m,:])] = vgmm.gmm_model.weights_[k]
        else:
          vgmm_tau[str(tau[m,:])] += vgmm.gmm_model.weights_[k]
        wrong_reconstruction = 0
    if wrong_reconstruction:
      vgmm_err += vgmm.gmm_model.weights_[k]
      if str(mu) not in vgmm_tau.keys():
        vgmm_tau[str(mu)] = vgmm.gmm_model.weights_[k]
      else:
        vgmm_tau[str(mu)] += vgmm.gmm_model.weights_[k]
  rec_err_vgmm.append(vgmm_err)
  M_eff_vgmm[len(list(vgmm_tau.values()))-1] += 1
  weights_eff_vgmm[:len(list(vgmm_tau.values())),test_num] = list(vgmm_tau.values())

  # GMM + AE
  mu_gmm_DEC_decoded_projected = np.round(scaler.inverse_transform( ae.decoder.predict(gmm_DEC.gmm_model.means_))).astype(int)
  gmm_DEC2tau = np.zeros(M,dtype=int) # how many clusters are projected onto the true vertices
  gmm_DEC_err = 0 # how many new (fake) clusters are created
  gmm_DEC_tau = {} # dictionary of reconstructed hypercube vertices (keys) and corresponding weights (values)
  for k in range(len(mu_gmm_DEC_decoded_projected)):
    mu = mu_gmm_DEC_decoded_projected[k]
    wrong_reconstruction = 1
    for m in range(M):
      if np.count_nonzero(tau[m,:] != mu) == 0:
        gmm_DEC2tau[m] += 1
        if str(tau[m,:]) not in gmm_DEC_tau.keys():
          gmm_DEC_tau[str(tau[m,:])] = gmm_DEC.gmm_model.weights_[k]
        else:
          gmm_DEC_tau[str(tau[m,:])] += gmm_DEC.gmm_model.weights_[k]
        wrong_reconstruction = 0
    if wrong_reconstruction:
      gmm_DEC_err += gmm_DEC.gmm_model.weights_[k]
      if str(mu) not in gmm_DEC_tau.keys():
        gmm_DEC_tau[str(mu)] = gmm_DEC.gmm_model.weights_[k]
      else:
        gmm_DEC_tau[str(mu)] += gmm_DEC.gmm_model.weights_[k]
  rec_err_gmm_DEC.append(gmm_DEC_err)
  M_eff_gmm_DEC[len(list(gmm_DEC_tau.values()))-1] += 1
  weights_eff_gmm_DEC[:len(list(gmm_DEC_tau.values())),test_num] = list(gmm_DEC_tau.values())

  # V-GMM + AE
  mu_vgmm_DEC_decoded_projected = np.round(scaler.inverse_transform( ae.decoder.predict(vgmm_DEC.gmm_model.means_))).astype(int) 
  vgmm_DEC2tau = np.zeros(M,dtype=int) # how many clusters are projected onto the true vertices
  vgmm_DEC_err = 0 # how many new (fake) clusters are created
  vgmm_DEC_tau = {} # dictionary of reconstructed hypercube vertices (keys) and corresponding weights (values)
  for k in range(len(mu_vgmm_DEC_decoded_projected)):
    mu = mu_vgmm_DEC_decoded_projected[k]
    wrong_reconstruction = 1
    for m in range(M):
      if np.count_nonzero(tau[m,:] != mu) == 0:
        vgmm_DEC2tau[m] += 1
        if str(tau[m,:]) not in vgmm_DEC_tau.keys():
          vgmm_DEC_tau[str(tau[m,:])] = vgmm_DEC.gmm_model.weights_[k]
        else:
          vgmm_DEC_tau[str(tau[m,:])] += vgmm_DEC.gmm_model.weights_[k]
        wrong_reconstruction = 0
    if wrong_reconstruction:
      vgmm_DEC_err += vgmm_DEC.gmm_model.weights_[k]
      if str(mu) not in vgmm_DEC_tau.keys():
        vgmm_DEC_tau[str(mu)] = vgmm_DEC.gmm_model.weights_[k]
      else:
        vgmm_DEC_tau[str(mu)] += vgmm_DEC.gmm_model.weights_[k]
  rec_err_vgmm_DEC.append(vgmm_DEC_err)
  M_eff_vgmm_DEC[len(list(vgmm_DEC_tau.values()))-1] += 1
  weights_eff_vgmm_DEC[:len(list(vgmm_DEC_tau.values())),test_num] = list(vgmm_DEC_tau.values())

# CLUSTERING PERFORMANCE

# ESTIMATED NUMBER OF EVENTS
# pie-chart relative to the number of clusters estimated
# GMM
fig = plt.figure(figsize=(6,6))
ax1 = plt.subplot(2,2,1)
ax1.pie(M_gmm[np.nonzero(M_gmm)[0]], labels=np.nonzero(M_gmm)[0]+1,autopct='%1.1f%%', startangle=90,textprops={'fontsize': 14})
plt.title('GMM', fontsize=18)
ax1.axis('equal') 
# V-GMM
ax2 = plt.subplot(2,2,2)
ax2.pie(M_vgmm[np.nonzero(M_vgmm)[0]], labels=np.nonzero(M_vgmm)[0]+1,autopct='%1.1f%%', startangle=90,textprops={'fontsize': 14})
ax2.axis('equal')
plt.title('V-GMM', fontsize=18) 
plt.tight_layout()
# GMM+AE
ax3 = plt.subplot(2,2,3)
ax3.pie(M_gmm_DEC[np.nonzero(M_gmm_DEC)[0]], labels=np.nonzero(M_gmm_DEC)[0]+1,autopct='%1.1f%%', startangle=90,textprops={'fontsize': 14})
ax3.axis('equal')
plt.title('GMM+AE', fontsize=18) 
plt.tight_layout()
# V-GMM+AE
ax4 = plt.subplot(2,2,4)
ax4.pie(M_vgmm_DEC[np.nonzero(M_vgmm_DEC)[0]], labels=np.nonzero(M_vgmm_DEC)[0]+1,autopct='%1.1f%%', startangle=90,textprops={'fontsize': 14})
ax4.axis('equal')
plt.title('V-GMM+AE', fontsize=18) 
plt.tight_layout()
plt.show()

# bar chart of the weights given to each component
# GMM
fig = plt.figure(figsize=(15,12))
ax1 = plt.subplot(4,1,1)
gmm_avg_weights = np.mean(weights_gmm, axis= 1)
gmm_std_weights = np.std(weights_gmm, axis= 1)
ax1.bar(range(1,M_max+1),gmm_avg_weights, yerr=gmm_std_weights,align='center', color='g', alpha=0.5, ecolor='black', capsize=10)
ax1.set_xticks(range(1,M_max+1))
plt.xlabel(r'$m$',fontsize=18)
plt.ylabel(r'$\pi_m$',fontsize=18)
plt.title('GMM', fontsize=18)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
# V-GMM
ax2 = plt.subplot(4,1,2)
vgmm_avg_weights = np.mean(weights_vgmm, axis= 1)
vgmm_std_weights = np.std(weights_vgmm, axis= 1)
ax2.bar(range(1,M_max+1),vgmm_avg_weights, yerr=vgmm_std_weights,align='center', color='b', alpha=0.5, ecolor='black', capsize=10)
ax2.set_xticks(range(1,M_max+1))
plt.xlabel(r'$m$',fontsize=18)
plt.ylabel(r'$\pi_m$',fontsize=18)
plt.title('V-GMM', fontsize=18)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.tight_layout()
# GMM+AE
ax3 = plt.subplot(4,1,3)
gmm_DEC_avg_weights = np.mean(weights_gmm_DEC, axis= 1)
gmm_DEC_std_weights = np.std(weights_gmm_DEC, axis= 1)
ax3.bar(range(1,M_max+1), gmm_DEC_avg_weights, yerr=gmm_DEC_std_weights,align='center', color='y', alpha=0.5, ecolor='black', capsize=10)
ax3.set_xticks(range(1,M_max+1))
plt.xlabel(r'$m$',fontsize=18)
plt.ylabel(r'$\pi_m$',fontsize=18)
plt.title('GMM+AE', fontsize=18)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.tight_layout()
# V-GMM+AE
ax4 = plt.subplot(4,1,4)
vgmm_DEC_avg_weights = np.mean(weights_vgmm_DEC, axis= 1)
vgmm_DEC_std_weights = np.std(weights_vgmm_DEC, axis= 1)
ax4.bar(range(1,M_max+1),vgmm_DEC_avg_weights, yerr=vgmm_DEC_std_weights,align='center', color='r', alpha=0.5, ecolor='black', capsize=10)
ax4.set_xticks(range(1,M_max+1))
plt.xlabel(r'$m$',fontsize=18)
plt.ylabel(r'$\pi_m$',fontsize=18)
plt.title('V-GMM+AE', fontsize=18)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.tight_layout()

# ACCURACY
# ecdf of the accuracies
fig = plt.figure(figsize=(15,5))
# GMM
ecdf_gmm = ECDF(acc_gmm_MC)
plt.plot(ecdf_gmm.x, ecdf_gmm.y, label='GMM', c='g',linewidth=3)
# V-GMM
ecdf_vgmm = ECDF(acc_vgmm_MC)
plt.plot(ecdf_vgmm.x, ecdf_vgmm.y, label='V-GMM', c='b',linewidth=3)
# GMM+AE
ecdf_gmm_DEC = ECDF(acc_gmm_DEC_MC)
plt.plot(ecdf_gmm_DEC.x, ecdf_gmm_DEC.y, label='GMM+AE', c='y',linewidth=3)
# V-GMM+AE
ecdf_vgmm_DEC = ECDF(acc_vgmm_DEC_MC)
plt.plot(ecdf_vgmm_DEC.x, ecdf_vgmm_DEC.y, label='V-GMM+AE', c='r',linewidth=3)
plt.legend(loc='upper left', fontsize=18)
plt.xlabel('acc',fontsize=18)
plt.ylabel('ECDF',fontsize=18)
plt.yticks(fontsize=18)
plt.xticks(fontsize=18)
plt.ylim([0,1.01])
plt.show()

# REPROJECTION PERFORMANCE

# ESTIMATED NUMBER OF EFFECTIVE EVENTS
# pie-chart relative to the effective number of clusters 
# GMM
fig = plt.figure(figsize=(6,6))
ax1 = plt.subplot(2,2,1)
ax1.pie(M_eff_gmm[np.nonzero(M_eff_gmm)[0]], labels=np.nonzero(M_eff_gmm)[0]+1,autopct='%1.1f%%', startangle=90,textprops={'fontsize': 14})
plt.title('GMM', fontsize=18)
ax1.axis('equal') 
# V-GMM
ax2 = plt.subplot(2,2,2)
ax2.pie(M_eff_vgmm[np.nonzero(M_eff_vgmm)[0]], labels=np.nonzero(M_eff_vgmm)[0]+1,autopct='%1.1f%%', startangle=90,textprops={'fontsize': 14})
ax2.axis('equal')
plt.title('V-GMM', fontsize=18) 
plt.tight_layout()
# GMM+AE
ax3 = plt.subplot(2,2,3)
ax3.pie(M_eff_gmm_DEC[np.nonzero(M_eff_gmm_DEC)[0]], labels=np.nonzero(M_eff_gmm_DEC)[0]+1,autopct='%1.1f%%', startangle=90,textprops={'fontsize': 14})
ax3.axis('equal')
plt.title('GMM+AE', fontsize=18) 
plt.tight_layout()
# V-GMM+AE
ax4 = plt.subplot(2,2,4)
ax4.pie(M_eff_vgmm_DEC[np.nonzero(M_eff_vgmm_DEC)[0]], labels=np.nonzero(M_eff_vgmm_DEC)[0]+1,autopct='%1.1f%%', startangle=90,textprops={'fontsize': 14})
ax4.axis('equal')
plt.title('V-GMM+AE', fontsize=18) 
plt.tight_layout()
plt.show()

# bar chart of the weights given to each component
# GMM
fig = plt.figure(figsize=(15,12))
ax1 = plt.subplot(4,1,1)
gmm_avg_weights_eff = np.mean(weights_eff_gmm, axis= 1)
gmm_std_weights_eff = np.std(weights_eff_gmm, axis= 1)
ax1.bar(range(1,M_max+1),gmm_avg_weights_eff, yerr=gmm_std_weights_eff,align='center', color='g', alpha=0.5, ecolor='black', capsize=10)
ax1.set_xticks(range(1,M_max+1))
plt.xlabel(r'$m$',fontsize=18)
plt.ylabel(r'$\pi_{m,eff}$',fontsize=18)
plt.title('GMM', fontsize=18)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.ylim([0,1])
# V-GMM
ax2 = plt.subplot(4,1,2)
vgmm_avg_weights_eff = np.mean(weights_eff_vgmm, axis= 1)
vgmm_std_weights_eff = np.std(weights_eff_vgmm, axis= 1)
ax2.bar(range(1,M_max+1),vgmm_avg_weights_eff, yerr=vgmm_std_weights_eff,align='center', color='b', alpha=0.5, ecolor='black', capsize=10)
ax2.set_xticks(range(1,M_max+1))
plt.xlabel(r'$m$',fontsize=18)
plt.ylabel(r'$\pi_{m,eff}$',fontsize=18)
plt.title('V-GMM', fontsize=18)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.ylim([0,1])
plt.tight_layout()
# GMM+AE
ax3 = plt.subplot(4,1,3)
gmm_DEC_avg_weights_eff = np.mean(weights_eff_gmm_DEC, axis= 1)
gmm_DEC_std_weights_eff = np.std(weights_eff_gmm_DEC, axis= 1)
ax3.bar(range(1,M_max+1), gmm_DEC_avg_weights_eff, yerr=gmm_DEC_std_weights_eff,align='center', color='y', alpha=0.5, ecolor='black', capsize=10)
ax3.set_xticks(range(1,M_max+1))
plt.xlabel(r'$m$',fontsize=18)
plt.ylabel(r'$\pi_{m,eff}$',fontsize=18)
plt.title('GMM+AE', fontsize=18)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.ylim([0,1])
plt.tight_layout()
# V-GMM+AE
ax4 = plt.subplot(4,1,4)
vgmm_DEC_avg_weights_eff = np.mean(weights_eff_vgmm_DEC, axis= 1)
vgmm_DEC_std_weights_eff = np.std(weights_eff_vgmm_DEC, axis= 1)
ax4.bar(range(1,M_max+1),vgmm_DEC_avg_weights_eff, yerr=vgmm_DEC_std_weights_eff,align='center', color='r', alpha=0.5, ecolor='black', capsize=10)
ax4.set_xticks(range(1,M_max+1))
plt.xlabel(r'$m$',fontsize=18)
plt.ylabel(r'$\pi_{m,eff}$',fontsize=18)
plt.title('V-GMM+AE', fontsize=18)
plt.xticks(fontsize=18)
plt.yticks(fontsize=18)
plt.ylim([0,1])
plt.tight_layout()

# RECONSTRUCTION ERROR
# ecdf of the reconstruction error
fig = plt.figure(figsize=(15,5))
# GMM
ecdf_gmm_err = ECDF(rec_err_gmm)
plt.plot(ecdf_gmm_err.x, ecdf_gmm_err.y, label='GMM', c='g',linewidth=3)
# V-GMM
ecdf_vgmm_err = ECDF(rec_err_vgmm)
plt.plot(ecdf_vgmm_err.x, ecdf_vgmm_err.y, label='V-GMM', c='b',linewidth=3)
# GMM+AE
ecdf_gmm_DEC_err = ECDF(rec_err_gmm_DEC)
plt.plot(ecdf_gmm_DEC_err.x, ecdf_gmm_DEC_err.y, label='GMM+AE', c='y',linewidth=3)
# V-GMM+AE
ecdf_vgmm_DEC_err = ECDF(rec_err_vgmm_DEC)
plt.plot(ecdf_vgmm_DEC_err.x, ecdf_vgmm_DEC_err.y, label='V-GMM+AE', c='r',linewidth=3)
plt.legend(loc='lower right', fontsize=18)
plt.xlabel(r'$e_r$',fontsize=18)
plt.ylabel('ECDF',fontsize=18)
plt.yticks(fontsize=18)
plt.ylim([0,1.01])
plt.show()